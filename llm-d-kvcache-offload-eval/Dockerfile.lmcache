FROM ghcr.io/llm-d/llm-d-cuda:v0.4.0

# Switch to root to install packages
USER root

# Install lmcache with CUDA architecture support for multiple GPU generations
# TORCH_CUDA_ARCH_LIST controls which GPU architectures PyTorch extensions are compiled for
# Supported architectures:
#   8.0 = Ampere (A100, A30, A10)
#   8.6 = Ampere (A40, RTX A6000)
#   8.9 = Ada Lovelace (L40S, L40, RTX 4090, RTX 6000 Ada)
#   9.0 = Hopper (H100, H200)
#   10.0 = Blackwell (B100, B200, GB200)
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;10.0"
RUN /opt/vllm/bin/python3 -m ensurepip && \
    /opt/vllm/bin/pip3 install --no-cache-dir lmcache

# Switch back to the original user (if any)
USER vllm
