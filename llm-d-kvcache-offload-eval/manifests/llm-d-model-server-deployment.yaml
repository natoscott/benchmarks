apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"llm-d-model-server","namespace":"llm-d-pfc-cpu"},"spec":{"replicas":1,"selector":{"matchLabels":{"llm-d.ai/inference-serving":"true"}},"template":{"metadata":{"labels":{"llm-d.ai/accelerator-vendor":"nvidia","llm-d.ai/hardware-variant":"gpu","llm-d.ai/inference-serving":"true"}},"spec":{"containers":[{"args":["exec vllm serve \\\n  Qwen/Qwen3-0.6B \\\n  --tensor-parallel-size 1 \\\n  --kv-transfer-config '{\"kv_connector\":\"OffloadingConnector\",\"kv_role\":\"kv_both\",\"kv_connector_extra_config\":{\"num_cpu_blocks\":10000}}' \\\n  --port 8000 \\\n  --max-num-seq 1024"],"command":["/bin/bash","-c"],"env":[{"name":"HUGGING_FACE_HUB_TOKEN","valueFrom":{"secretKeyRef":{"key":"HF_TOKEN","name":"llm-d-hf-token"}}},{"name":"HF_HOME","value":"/data/.hf"}],"image":"ghcr.io/llm-d/llm-d-cuda:v0.4.0","imagePullPolicy":"IfNotPresent","lifecycle":{"preStop":{"sleep":{"seconds":30}}},"livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/health","port":"http","scheme":"HTTP"},"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"name":"vllm","ports":[{"containerPort":8000,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":1,"httpGet":{"path":"/health","port":"http","scheme":"HTTP"},"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"resources":{"limits":{"nvidia.com/gpu":2},"requests":{"memory":"200Gi","nvidia.com/gpu":2}},"startupProbe":{"failureThreshold":600,"httpGet":{"path":"/health","port":"http","scheme":"HTTP"},"initialDelaySeconds":2,"periodSeconds":1},"volumeMounts":[{"mountPath":"/data","name":"data"},{"mountPath":"/dev/shm","name":"shm"}]}],"enableServiceLinks":false,"restartPolicy":"Always","terminationGracePeriodSeconds":130,"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"llm-d-model-cache"}},{"emptyDir":{"medium":"Memory"},"name":"shm"}]}}}}
  creationTimestamp: "2026-02-11T04:22:11Z"
  generation: 3
  name: llm-d-model-server
  namespace: llm-d-pfc-cpu
  resourceVersion: "383527"
  uid: d2cc2f6e-a202-4509-9cb8-4b78eef21445
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        llm-d.ai/accelerator-vendor: nvidia
        llm-d.ai/hardware-variant: gpu
        llm-d.ai/inference-serving: "true"
    spec:
      containers:
      - args:
        - exec vllm serve Qwen/Qwen3-0.6B --tensor-parallel-size 1 --port 8000 --max-num-seq
          1024
        command:
        - /bin/bash
        - -c
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              key: HF_TOKEN
              name: llm-d-hf-token
        - name: HF_HOME
          value: /data/.hf
        image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            sleep:
              seconds: 30
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 1
        name: vllm
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            nvidia.com/gpu: "2"
          requests:
            memory: 200Gi
            nvidia.com/gpu: "2"
        startupProbe:
          failureThreshold: 600
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          initialDelaySeconds: 2
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /dev/shm
          name: shm
      dnsPolicy: ClusterFirst
      enableServiceLinks: false
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 130
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: llm-d-model-cache
      - emptyDir:
          medium: Memory
        name: shm
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2026-02-12T04:34:52Z"
    lastUpdateTime: "2026-02-12T04:34:52Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2026-02-11T05:06:12Z"
    lastUpdateTime: "2026-02-12T04:34:52Z"
    message: ReplicaSet "llm-d-model-server-674b7cd58b" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 3
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
